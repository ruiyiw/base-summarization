Could not find conda environment: fairseq
You can list all discoverable environments with `conda info --envs`.

2021-04-28 16:43:10 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10612
2021-04-28 16:43:10 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10612
2021-04-28 16:43:11 | INFO | root | Added key: store_based_barrier_key:1 to store for rank: 1
2021-04-28 16:43:11 | INFO | root | Added key: store_based_barrier_key:1 to store for rank: 0
2021-04-28 16:43:11 | INFO | fairseq.distributed_utils | initialized host sccdlb033 as rank 0
2021-04-28 16:43:11 | INFO | fairseq.distributed_utils | initialized host sccdlb033 as rank 1
2021-04-28 16:43:18 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/share03/rwang/fairseq-1.0/data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10612', distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args=None, eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2021-04-28 16:43:18 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2021-04-28 16:43:18 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2021-04-28 16:43:18 | INFO | fairseq.data.data_utils | loaded 7283 examples from: /share03/rwang/fairseq-1.0/data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2021-04-28 16:43:18 | INFO | fairseq.data.data_utils | loaded 7283 examples from: /share03/rwang/fairseq-1.0/data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2021-04-28 16:43:18 | INFO | fairseq.tasks.translation | /share03/rwang/fairseq-1.0/data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
Traceback (most recent call last):
  File "/share03/rwang/anaconda3/bin/fairseq-train", line 8, in <module>
    sys.exit(cli_main())
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq_cli/train.py", line 352, in cli_main
    distributed_utils.call_main(args, main)
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/distributed_utils.py", line 283, in call_main
    torch.multiprocessing.spawn(
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/data/encoders/moses_tokenizer.py", line 33, in __init__
    from sacremoses import MosesTokenizer, MosesDetokenizer
ModuleNotFoundError: No module named 'sacremoses'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/distributed_utils.py", line 270, in distributed_main
    main(args, **kwargs)
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq_cli/train.py", line 68, in main
    model = task.build_model(args)
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/tasks/translation.py", line 335, in build_model
    self.tokenizer = encoders.build_tokenizer(
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/registry.py", line 54, in build_x
    return builder(args, *extra_args, **extra_kwargs)
  File "/share03/rwang/anaconda3/lib/python3.8/site-packages/fairseq/data/encoders/moses_tokenizer.py", line 38, in __init__
    raise ImportError(
ImportError: Please install Moses tokenizer with: pip install sacremoses

